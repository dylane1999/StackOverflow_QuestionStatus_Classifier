{"nbformat":4,"nbformat_minor":0,"metadata":{"interpreter":{"hash":"2972d6e7ffecf54ed5e842bd2e14961f37161ce7e144cb9624e826c965cf2420"},"kernelspec":{"display_name":"Python 3.8.0 64-bit ('venv': venv)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"name":"baseline-multiclass-logisticRegression.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"KL31Cxe1RSDA"},"source":["# Logistic Regression with uni, bi, and tri gram embeddings"]},{"cell_type":"markdown","metadata":{"id":"hr8e30BXRSDD"},"source":["## imports"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"G0U40X0ARSDE","executionInfo":{"status":"ok","timestamp":1638318023931,"user_tz":480,"elapsed":141,"user":{"displayName":"Dylan Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXQ_dE6d9WUzT6Gktm7cvNb_NHfDvzIOyWGESgwg=s64","userId":"04593832962814526698"}},"outputId":"6ddce377-4085-48c3-a6ad-5dba6cde5d36"},"source":["import string\n","from os import listdir\n","from os.path import isfile, join\n","import nltk\n","from nltk import word_tokenize\n","from nltk.corpus import opinion_lexicon\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","import re\n","import numpy as np\n","import pandas as pd\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import KFold\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import average_precision_score\n","from sklearn.metrics import plot_precision_recall_curve\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","\n","# Libraries\n","\n","import matplotlib.pyplot as plt\n","\n","import pandas as pd\n","# Preliminaries\n","from sklearn.model_selection import train_test_split\n","\n","# Models\n","\n","# Training\n","\n","# Evaluation\n","\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","nltk.download('opinion_lexicon')\n","nltk.download(\"stopwords\")\n","nltk.download(\"punkt\")"],"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package opinion_lexicon to /root/nltk_data...\n","[nltk_data]   Package opinion_lexicon is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":55}]},{"cell_type":"markdown","metadata":{"id":"fpHMwbUqRSDG"},"source":["### configs"]},{"cell_type":"markdown","metadata":{"id":"3ZnvWujMRSDH"},"source":["### setup puncuation and contractions to help clean text"]},{"cell_type":"code","metadata":{"id":"_uMuh9JuRSDH","executionInfo":{"status":"ok","timestamp":1638318026339,"user_tz":480,"elapsed":456,"user":{"displayName":"Dylan Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXQ_dE6d9WUzT6Gktm7cvNb_NHfDvzIOyWGESgwg=s64","userId":"04593832962814526698"}}},"source":["punctuation = string.punctuation\n","\n","contractions = {\n","\"ain't\": \"am not\",\n","\"aren't\": \"are not\",\n","\"can't\": \"cannot\",\n","\"can't've\": \"cannot have\",\n","\"'cause\": \"because\",\n","\"could've\": \"could have\",\n","\"couldn't\": \"could not\",\n","\"couldn't've\": \"could not have\",\n","\"didn't\": \"did not\",\n","\"doesn't\": \"does not\",\n","\"don't\": \"do not\",\n","\"hadn't\": \"had not\",\n","\"hadn't've\": \"had not have\",\n","\"hasn't\": \"has not\",\n","\"haven't\": \"have not\",\n","\"he'd\": \"he would\",\n","\"he'd've\": \"he would have\",\n","\"he'll\": \"he will\",\n","\"he's\": \"he is\",\n","\"how'd\": \"how did\",\n","\"how'll\": \"how will\",\n","\"how's\": \"how is\",\n","\"i'd\": \"i would\",\n","\"i'll\": \"i will\",\n","\"i'm\": \"i am\",\n","\"i've\": \"i have\",\n","\"isn't\": \"is not\",\n","\"it'd\": \"it would\",\n","\"it'll\": \"it will\",\n","\"it's\": \"it is\",\n","\"let's\": \"let us\",\n","\"ma'am\": \"madam\",\n","\"mayn't\": \"may not\",\n","\"might've\": \"might have\",\n","\"mightn't\": \"might not\",\n","\"must've\": \"must have\",\n","\"mustn't\": \"must not\",\n","\"needn't\": \"need not\",\n","\"oughtn't\": \"ought not\",\n","\"shan't\": \"shall not\",\n","\"sha'n't\": \"shall not\",\n","\"she'd\": \"she would\",\n","\"she'll\": \"she will\",\n","\"she's\": \"she is\",\n","\"should've\": \"should have\",\n","\"shouldn't\": \"should not\",\n","\"that'd\": \"that would\",\n","\"that's\": \"that is\",\n","\"there'd\": \"there had\",\n","\"there's\": \"there is\",\n","\"they'd\": \"they would\",\n","\"they'll\": \"they will\",\n","\"they're\": \"they are\",\n","\"they've\": \"they have\",\n","\"wasn't\": \"was not\",\n","\"we'd\": \"we would\",\n","\"we'll\": \"we will\",\n","\"we're\": \"we are\",\n","\"we've\": \"we have\",\n","\"weren't\": \"were not\",\n","\"what'll\": \"what will\",\n","\"what're\": \"what are\",\n","\"what's\": \"what is\",\n","\"what've\": \"what have\",\n","\"where'd\": \"where did\",\n","\"where's\": \"where is\",\n","\"who'll\": \"who will\",\n","\"who's\": \"who is\",\n","\"won't\": \"will not\",\n","\"wouldn't\": \"would not\",\n","\"you'd\": \"you would\",\n","\"you'll\": \"you will\",\n","\"you're\": \"you are\"\n","}"],"execution_count":56,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W7_JaXWPRSDI"},"source":["### functions to help get data from txt files and to process and clean data"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"mpvOIPBlRSDJ","executionInfo":{"status":"ok","timestamp":1638316728879,"user_tz":480,"elapsed":457,"user":{"displayName":"Dylan Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXQ_dE6d9WUzT6Gktm7cvNb_NHfDvzIOyWGESgwg=s64","userId":"04593832962814526698"}}},"source":["def get_file_key(fileName):\n","    # get the file num\n","    return int(fileName.split(\"_\")[1].split(\".\")[0]) \n","    \n","def get_files_from_dir(directory):\n","    filesInDir = [f for f in listdir(directory) if isfile(join(directory, f))]\n","    return sorted(filesInDir, key = get_file_key)   \n","        \n","def process_string_sentence(text):\n","        englishStopwords = stopwords.words(\"english\")  # non-neccesary words\n","        text = text.lower()  # case folding\n","        # remove punctuation\n","        text = \"\".join([char for char in text if char not in punctuation])\n","        words = word_tokenize(text)\n","        removed = [word for word in words if word not in englishStopwords]\n","        stemmed = [PorterStemmer().stem(word) for word in removed]\n","        stemmed_sentence = \" \".join(stemmed)\n","        return stemmed_sentence\n","\n","def process_string(text):\n","        englishStopwords = stopwords.words(\"english\")  # non-neccesary words\n","        text = text.lower()  # case folding\n","        # remove punctuation\n","        text = \"\".join([char for char in text if char not in punctuation])\n","        words = word_tokenize(text)\n","        removed = [word for word in words if word not in englishStopwords]\n","        return [\", \".join(removed)]\n","\n","\n","def tokenize_files(files, dir):\n","        cleaned_positive_files = []\n","        for file in files:\n","            file_path = str.format(\"{}/{}\", dir, file)\n","            with open(file_path) as f:\n","                raw_text = f.read()\n","                cleaned_positive_files.append(process_string(raw_text))\n","        return cleaned_positive_files\n","\n","def is_word_positive(word):\n","        if word in positive_dict or word in positive_dict_stemmed:\n","            return True\n","        return False\n","\n","def is_word_negative(word):\n","    if word in negative_dict or word in negative_dict_stemmed:\n","        return True\n","    return False\n","\n","\n","\n","def get_word_occurrences(tokenized_files):\n","        word_occurrences = {}\n","        word_occurrences[\"positive\"] = 0\n","        word_occurrences[\"negative\"] = 0\n","        total_num_words = 0\n","        for file in tokenized_files:\n","            # calc number exclams\n","            # calc number pos/neg/words\n","            for word in file:\n","                if is_word_positive(word):\n","                    word_occurrences[\"positive\"] += 1\n","                if is_word_negative(word):\n","                    word_occurrences[\"negative\"] += 1\n","                if word not in word_occurrences:\n","                    word_occurrences[word] = 0\n","                word_occurrences[word] += 1\n","                total_num_words += 1\n","        return word_occurrences, total_num_words\n","\n","def get_raw_text_from_files(files: list, dir: str) -> list:\n","    raw_text = []\n","    for file in files:\n","        file_path = str.format(\"{}/{}\", dir, file)\n","        with open(file_path) as f:\n","            file_text_in_lines = f.read()\n","            raw_text.append(file_text_in_lines)\n","    return raw_text\n","\n","def clean_text(text, remove_stopwords = True):\n","    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n","\n","    # Convert words to lower case\n","    text = text.lower()\n","\n","    # Replace contractions with their longer forms\n","    if True:\n","        text = text.split()\n","        new_text = []\n","        for word in text:\n","            if word in contractions:\n","                new_text.append(contractions[word])\n","            else:\n","                new_text.append(word)\n","        text = \" \".join(new_text)\n","\n","    # Format words and remove unwanted characters\n","    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\<a href', ' ', text)\n","    text = re.sub(r'&amp;', '', text)\n","    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n","    text = re.sub(r'<br />', ' ', text)\n","    text = re.sub(r'\\'', ' ', text)\n","\n","    # remove stop words\n","    if remove_stopwords:\n","        text = text.split()\n","        stops = set(stopwords.words(\"english\"))\n","        text = [w for w in text if not w in stops]\n","        text = \" \".join(text)\n","\n","    # Tokenize each word\n","    text =  nltk.WordPunctTokenizer().tokenize(text)\n","\n","    return text\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YIj7b6GRRSDK"},"source":["### import and process the data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_6SuzlbiRgLC","executionInfo":{"status":"ok","timestamp":1638316761137,"user_tz":480,"elapsed":28965,"user":{"displayName":"Dylan Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXQ_dE6d9WUzT6Gktm7cvNb_NHfDvzIOyWGESgwg=s64","userId":"04593832962814526698"}},"outputId":"eee19c06-858c-4610-993b-0307fb166a0d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":496},"id":"4PvzEFDlRSDL","executionInfo":{"status":"ok","timestamp":1638318033899,"user_tz":480,"elapsed":1369,"user":{"displayName":"Dylan Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXQ_dE6d9WUzT6Gktm7cvNb_NHfDvzIOyWGESgwg=s64","userId":"04593832962814526698"}},"outputId":"512ca0d5-41a7-4704-d866-b7459d659c16"},"source":["# Import the csv into pandas dataframe and add the headers\n","df = pd.read_csv('/content/drive/MyDrive/stack_nlp_large.csv')\n","df.head()"],"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>PostId</th>\n","      <th>PostCreationDate</th>\n","      <th>OwnerUserId</th>\n","      <th>OwnerCreationDate</th>\n","      <th>ReputationAtPostCreation</th>\n","      <th>OwnerUndeletedAnswerCountAtPostTime</th>\n","      <th>Title</th>\n","      <th>BodyMarkdown</th>\n","      <th>Tag1</th>\n","      <th>Tag2</th>\n","      <th>Tag3</th>\n","      <th>Tag4</th>\n","      <th>Tag5</th>\n","      <th>PostClosedDate</th>\n","      <th>OpenStatus</th>\n","      <th>titletext</th>\n","      <th>closed_reason_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7</td>\n","      <td>23</td>\n","      <td>08/01/2008 12:09:41</td>\n","      <td>48</td>\n","      <td>08/01/2008 13:25:15</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>Latest information on PHP upcoming releases</td>\n","      <td>I'm trying to track the progress of PHP 5.3 an...</td>\n","      <td>php</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>05/18/2012 11:12:42</td>\n","      <td>not constructive</td>\n","      <td>Latest information on PHP upcoming releases. I...</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>30</td>\n","      <td>126</td>\n","      <td>08/01/2008 16:10:30</td>\n","      <td>58</td>\n","      <td>08/01/2008 13:56:33</td>\n","      <td>11</td>\n","      <td>1</td>\n","      <td>How would you access Object properties from wi...</td>\n","      <td>What is the \"purist\" or \"correct\" way to acces...</td>\n","      <td>oo</td>\n","      <td>java</td>\n","      <td>php</td>\n","      <td>theory</td>\n","      <td>NaN</td>\n","      <td>05/08/2012 18:11:27</td>\n","      <td>not constructive</td>\n","      <td>How would you access Object properties from wi...</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>31</td>\n","      <td>129</td>\n","      <td>08/01/2008 16:22:42</td>\n","      <td>48</td>\n","      <td>08/01/2008 13:25:15</td>\n","      <td>11</td>\n","      <td>1</td>\n","      <td>How to export data from SQL Server to MySQL</td>\n","      <td>I've been banging my head against SQL Server 2...</td>\n","      <td>csv</td>\n","      <td>ansi</td>\n","      <td>sql</td>\n","      <td>php</td>\n","      <td>mssql</td>\n","      <td>07/03/2012 14:30:16</td>\n","      <td>off topic</td>\n","      <td>How to export data from SQL Server to MySQL. I...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>37</td>\n","      <td>173</td>\n","      <td>08/01/2008 18:33:08</td>\n","      <td>83</td>\n","      <td>08/01/2008 16:31:56</td>\n","      <td>16</td>\n","      <td>4</td>\n","      <td>How do I version my MS SQL database in SVN?</td>\n","      <td>I've been wanting to get my databases under ve...</td>\n","      <td>subversion</td>\n","      <td>svn</td>\n","      <td>sql</td>\n","      <td>mssql</td>\n","      <td>versioncontrol</td>\n","      <td>06/29/2012 15:08:28</td>\n","      <td>not constructive</td>\n","      <td>How do I version my MS SQL database in SVN?. I...</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>41</td>\n","      <td>177</td>\n","      <td>08/01/2008 18:37:55</td>\n","      <td>83</td>\n","      <td>08/01/2008 16:31:56</td>\n","      <td>16</td>\n","      <td>4</td>\n","      <td>How do I programmatically create a PDF in my ....</td>\n","      <td>Please recommend a good library for programmat...</td>\n","      <td>pdf</td>\n","      <td>.net</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>04/25/2012 11:32:29</td>\n","      <td>not constructive</td>\n","      <td>How do I programmatically create a PDF in my ....</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  ...  closed_reason_label\n","0           7  ...                    4\n","1          30  ...                    4\n","2          31  ...                    3\n","3          37  ...                    4\n","4          41  ...                    4\n","\n","[5 rows x 18 columns]"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","metadata":{"id":"zBHcqvBbRoEa","executionInfo":{"status":"ok","timestamp":1638318035504,"user_tz":480,"elapsed":128,"user":{"displayName":"Dylan Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXQ_dE6d9WUzT6Gktm7cvNb_NHfDvzIOyWGESgwg=s64","userId":"04593832962814526698"}}},"source":["# # Converting the codes to appropriate categories using a dictionary\n","def convertReasonToLabel(reason):\n","    mapper = {\"not a real question\": 0, \"too localized\": 1, \"off topic\": 2, 'not constructive': 3, \"open\": 4 }\n","    return mapper.get(reason)\n","\n","df['category'] = df['OpenStatus'].apply(convertReasonToLabel)\n","\n","# remove all open \n","df = df[df.category != 4]\n","\n","# df.head()"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"tQAR0mPCRSDL","executionInfo":{"status":"ok","timestamp":1638318040298,"user_tz":480,"elapsed":121,"user":{"displayName":"Dylan Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXQ_dE6d9WUzT6Gktm7cvNb_NHfDvzIOyWGESgwg=s64","userId":"04593832962814526698"}}},"source":["dataSample = df.sample(n = 5000)"],"execution_count":59,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ga6PR9bARSDL","executionInfo":{"status":"ok","timestamp":1638318041165,"user_tz":480,"elapsed":128,"user":{"displayName":"Dylan Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXQ_dE6d9WUzT6Gktm7cvNb_NHfDvzIOyWGESgwg=s64","userId":"04593832962814526698"}}},"source":["\n","dataSample['titletext'] = dataSample['Title'] + \". \" + dataSample['BodyMarkdown']"],"execution_count":60,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UFzVIwt8RSDM"},"source":["### gather and clean data "]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"6L5CUFdERSDM","executionInfo":{"status":"ok","timestamp":1638318043573,"user_tz":480,"elapsed":130,"user":{"displayName":"Dylan Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXQ_dE6d9WUzT6Gktm7cvNb_NHfDvzIOyWGESgwg=s64","userId":"04593832962814526698"}}},"source":["\n","dataSample['Titles_Cleaned'] = list(map(clean_text, dataSample[\"Title\"]))\n","dataSample['Bodies_Cleaned'] = list(map(clean_text, dataSample[\"BodyMarkdown\"]))\n","dataSample['Title_Text_Cleaned'] = list(map(clean_text, dataSample[\"titletext\"]))\n"],"execution_count":61,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W7NBH1xiRSDO"},"source":["### function to run logistic regression model with the given bow converter and data"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"ONmMLlEvRSDO","executionInfo":{"status":"ok","timestamp":1638318045241,"user_tz":480,"elapsed":130,"user":{"displayName":"Dylan Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXQ_dE6d9WUzT6Gktm7cvNb_NHfDvzIOyWGESgwg=s64","userId":"04593832962814526698"}}},"source":["import sklearn\n","\n","def run_logisitc(allData, allLabels, description,  bow_converter, _C=1.0,):\n","    kf = KFold(n_splits=2, shuffle=True)\n","    model = None\n","    for trainingIndex, testingIndex in kf.split(allData):\n","        # get the train/test labels and data from split\n","        trainingData, testingData = allData[trainingIndex], allData[testingIndex]\n","        trainingLabels, testingLabels = allLabels[trainingIndex], allLabels[testingIndex]\n","\n","        # convert the data to bow\n","        trainingData = bow_converter.fit_transform(trainingData)\n","        testingData = bow_converter.transform(testingData)\n","\n","        # create the model\n","        model = LogisticRegression(multi_class='ovr', solver='liblinear').fit(trainingData, trainingLabels)\n","\n","        # get the model score\n","        score = model.score(testingData, testingLabels)\n","        print(description, \" model score: \", score)\n","\n","        # get label prediction\n","        labelPrediction = model.predict(testingData)\n","        # print('Predicted value is =', lm.predict([X_test[200]]))\n","        print(\"f1 score (micro): \", sklearn.metrics.f1_score(testingLabels, labelPrediction, average='micro'))\n","        print(\"f1 score (macro): \", sklearn.metrics.f1_score(testingLabels, labelPrediction, average='macro'))\n","\n","    return model\n","\n"],"execution_count":62,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m6XZalaxRSDO","executionInfo":{"status":"ok","timestamp":1638318047146,"user_tz":480,"elapsed":117,"user":{"displayName":"Dylan Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXQ_dE6d9WUzT6Gktm7cvNb_NHfDvzIOyWGESgwg=s64","userId":"04593832962814526698"}},"outputId":"e780ec26-70f3-409e-8392-bf2b657791ee"},"source":["# turn into numpy array \n","allLabels = np.array(dataSample['category'])\n","allLabels.size"],"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5000"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9xn4wV1DRSDP","executionInfo":{"status":"ok","timestamp":1638318048624,"user_tz":480,"elapsed":116,"user":{"displayName":"Dylan Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXQ_dE6d9WUzT6Gktm7cvNb_NHfDvzIOyWGESgwg=s64","userId":"04593832962814526698"}},"outputId":"946ad0e1-6aa1-4464-c02b-519d49c01612"},"source":["# turn into numpy array \n","titlesAndText = np.array(dataSample['Title_Text_Cleaned'])\n","titlesAndText.size"],"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5000"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KLOSrlegRSDP","executionInfo":{"status":"ok","timestamp":1638319336512,"user_tz":480,"elapsed":4869,"user":{"displayName":"Dylan Edwards","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXQ_dE6d9WUzT6Gktm7cvNb_NHfDvzIOyWGESgwg=s64","userId":"04593832962814526698"}},"outputId":"98dd6984-6af6-40c0-b4d1-6c94ebc06652"},"source":["bigram_bow_converter = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False) \n","model = run_logisitc(titlesAndText, allLabels, \"bigram BOW\", bigram_bow_converter)"],"execution_count":71,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["bigram BOW  model score:  0.508\n","f1 score (micro):  0.508\n","f1 score (macro):  0.3866771510754241\n","bigram BOW  model score:  0.5036\n","f1 score (micro):  0.5036\n","f1 score (macro):  0.3778661438516365\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  ConvergenceWarning,\n"]}]}]}